{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goals:\n",
    "- hand-code the value iteration algorithm on a toy dataset\n",
    "- find the corresponding policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import basic libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a toy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4x4 state space S\n",
    "N = 4\n",
    "M = 4\n",
    "\n",
    "# Reward function (rewards given for getting in each of the states)\n",
    "R = np.array([[-1, -1, -1, -1],\n",
    "                  [-1, -1, -1, 100],\n",
    "                  [-1, -1, -100, -1],\n",
    "                  [-1, -1, -1, -1]])\n",
    "\n",
    "# action space A is { MOVE_LEFT, MOVE_RIGHT, MOVE_UP, MOVE_DOWN, STAY }\n",
    "LEFT = 0\n",
    "RIGHT = 1\n",
    "UP = 2\n",
    "DOWN = 3\n",
    "STAY = 4\n",
    "\n",
    "# when the agent decides to move in any direction, \n",
    "# it will actually move in that direction with 0.8 probability...\n",
    "p_direct = 0.8\n",
    "# ...and diverge to the \"lateral\" state with 0.1 probability\n",
    "p_diverge = 0.1\n",
    "# if there is a wall in a direction of the movement, \n",
    "# the agent will simply bounce of the wall and stay where it is\n",
    "\n",
    "# discount factor\n",
    "gamma = 0.90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start position\n",
    "start = (3, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bounce_if_hit_wall(new_pos, old_pos):\n",
    "    x, y = old_pos\n",
    "    x1, y1 = new_pos\n",
    "    \n",
    "    if x1 < 0 or x1 >= M or y1 < 0 or y1 >= N:\n",
    "        x1, y1 = x, y\n",
    "    \n",
    "    return x1, y1\n",
    "            \n",
    "\n",
    "def get_next_positions(action, pos):\n",
    "    x, y = pos\n",
    "\n",
    "    if action == LEFT:\n",
    "        next_pos_direct = x - 1, y\n",
    "        next_pos_diverge1 = x, y - 1\n",
    "        next_pos_diverge2 = x, y + 1\n",
    "    elif action == RIGHT:\n",
    "        next_pos_direct = x + 1, y\n",
    "        next_pos_diverge1 = x, y - 1\n",
    "        next_pos_diverge2 = x, y + 1\n",
    "    elif action == UP:\n",
    "        next_pos_direct = x, y - 1\n",
    "        next_pos_diverge1 = x - 1, y\n",
    "        next_pos_diverge2 = x + 1, y\n",
    "    elif action == DOWN:\n",
    "        next_pos_direct = x, y + 1\n",
    "        next_pos_diverge1 = x - 1, y\n",
    "        next_pos_diverge2 = x + 1, y\n",
    "        \n",
    "    next_pos_direct = bounce_if_hit_wall(next_pos_direct, pos)\n",
    "    next_pos_diverge1 = bounce_if_hit_wall(next_pos_diverge1, pos)\n",
    "    next_pos_diverge2 = bounce_if_hit_wall(next_pos_diverge2, pos)\n",
    "    \n",
    "    return next_pos_direct, next_pos_diverge1, next_pos_diverge2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_action_value(action, pos, V):\n",
    "    next_pos_direct, next_pos_diverge1, next_pos_diverge2 = get_next_positions(action, pos)\n",
    "    \n",
    "    x, y = next_pos_direct\n",
    "    x1, y1 = next_pos_diverge1\n",
    "    x2, y2 = next_pos_diverge2\n",
    "    \n",
    "    return gamma * (V[y, x] * p_direct + V[y1, x1] * p_diverge + V[y2, x2] * p_diverge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(V):\n",
    "    V_next = np.copy(V)\n",
    "    for y in range(N):\n",
    "        for x in range (M):\n",
    "            # negative states are sticky\n",
    "            if R[y, x] == -100:\n",
    "                V_next[y, x] = R[y, x] + gamma * V[y, x]\n",
    "            else:\n",
    "                value_stay = gamma * V[y, x]\n",
    "                value_left = get_next_action_value(LEFT, (x, y), V)\n",
    "                value_right = get_next_action_value(RIGHT, (x, y), V)\n",
    "                value_up = get_next_action_value(UP, (x, y), V)\n",
    "                value_down = get_next_action_value(DOWN, (x, y), V)\n",
    "                \n",
    "                V_next[y, x] = R[y, x] + max(value_stay, value_left, value_right, value_up, value_down)\n",
    "    return V_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations: 338\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  566.71576199,   652.67442452,   751.77047051,   864.46081576],\n",
       "       [  519.8417529 ,   596.21097272,   696.65934235,  1000.        ],\n",
       "       [  450.72704408,   409.72831133, -1000.        ,   691.20879121],\n",
       "       [  391.28497119,   361.62057835,   360.00607541,   581.39656754]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = np.zeros((N, M))\n",
    "cnt = 0\n",
    "\n",
    "V_prev = value_iteration(V)\n",
    "while(True):\n",
    "    V = value_iteration(V_prev)\n",
    "    cnt = cnt + 1\n",
    "    # if cnt == 1000: break;\n",
    "    if np.array_equal(V, V_prev): break;\n",
    "    V_prev = V\n",
    "    \n",
    "print(\"Iterations: \" + str(cnt))\n",
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_INF = -1000000\n",
    "\n",
    "def get_policy(V):\n",
    "    P = np.zeros((N, M), dtype='int16')\n",
    "    for y in range(N):\n",
    "        for x in range (M):\n",
    "            # negative states are sticky\n",
    "            if R[y, x] == -100:\n",
    "                P[y, x] = STAY\n",
    "            else:\n",
    "                if x > 0: left = V[y, x - 1]\n",
    "                else: left = MIN_INF\n",
    "\n",
    "                if x < M - 1: right = V[y, x + 1]\n",
    "                else: right = MIN_INF\n",
    "\n",
    "                if y > 0: up = V[y - 1, x]\n",
    "                else: up = MIN_INF\n",
    "\n",
    "                if y < N - 1: down = V[y + 1, x]\n",
    "                else: down = MIN_INF\n",
    "\n",
    "                stay = V[y, x]\n",
    "\n",
    "                action = np.argmax(np.array([left, right, up, down, stay]))\n",
    "                P[y, x] = action\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = get_policy(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['→', '→', '→', '↓'],\n",
       "       ['→', '→', '→', '↻'],\n",
       "       ['↑', '↑', '↻', '↑'],\n",
       "       ['↑', '↑', '→', '↑']], dtype='<U1')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_action(P):\n",
    "    if P == LEFT: return '←'\n",
    "    elif P == RIGHT: return '→'\n",
    "    elif P == UP: return '↑'\n",
    "    elif P == DOWN: return '↓'\n",
    "    elif P == STAY: return '↻'\n",
    "\n",
    "to_action = np.vectorize(to_action)\n",
    "to_action(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The value iteration algorithm converges after 338 iterations\n",
    "- The resulting policy mostly makes sense\n",
    "- One thing I don't get is why the algorithm is not taking the safest route ever, meaning walking along the upper boundary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
